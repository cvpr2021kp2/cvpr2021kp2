/home/tanyu/CVPR2021_KP2/data/cub200
/home/tanyu/CVPR2021_KP2/model_mit
Prepare the network and data.
DataParallel(
  (module): BCNN(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
    )
    (fc): Linear(in_features=9216, out_features=67, bias=True)
    (norm): runLayer()
  )
)
=> loading annotations from: TrainImagesnew.txt ...
=> loading annotations from: TestImagesnew.txt ...
Training.
Epoch	Train loss	Train acc	Test acc
./src/bilinear_cnn_fc_mit.py:57: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.fc.weight.data)
./src/bilinear_cnn_fc_mit.py:58: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w1.data)
./src/bilinear_cnn_fc_mit.py:59: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w2.data)
./src/bilinear_cnn_fc_mit.py:60: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w3.data)
./src/bilinear_cnn_fc_mit.py:61: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w4.data)
./src/bilinear_cnn_fc_mit.py:64: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  torch.nn.init.constant(self.fc.bias.data, val=0)
*1	3.963		14.00%		35.75%
*2	3.224		49.00%		54.93%
*3	2.625		66.00%		60.37%
*4	2.172		74.00%		65.97%
*5	1.849		78.00%		66.94%
*6	1.605		80.00%		68.58%
7	1.419		82.00%		68.06%
*8	1.273		84.00%		69.93%
9	1.156		85.00%		69.85%
*10	1.060		86.00%		70.37%
*11	0.982		87.00%		70.67%
*12	0.909		88.00%		72.31%
13	0.848		89.00%		72.31%
*14	0.795		90.00%		72.61%
15	0.752		90.00%		72.54%
16	0.710		91.00%		72.54%
*17	0.668		92.00%		73.36%
18	0.636		92.00%		72.84%
19	0.604		93.00%		72.76%
20	0.577		93.00%		73.21%
*21	0.547		94.00%		73.88%
22	0.531		94.00%		73.66%
*23	0.503		95.00%		74.70%
24	0.480		95.00%		74.25%
25	0.460		96.00%		74.10%
26	0.443		96.00%		74.48%
*27	0.424		96.00%		74.78%
28	0.408		96.00%		73.88%
*29	0.396		97.00%		75.60%
30	0.382		97.00%		75.07%
*31	0.366		97.00%		75.97%
32	0.353		97.00%		74.93%
33	0.344		97.00%		74.78%
34	0.331		98.00%		75.60%
35	0.321		98.00%		75.45%
36	0.312		98.00%		75.75%
37	0.299		98.00%		75.52%
38	0.291		98.00%		75.90%
*39	0.286		98.00%		76.12%
40	0.273		98.00%		75.60%
41	0.270		98.00%		75.60%
42	0.260		98.00%		75.37%
43	0.252		99.00%		76.12%
44	0.245		99.00%		75.75%
45	0.240		99.00%		75.67%
46	0.235		99.00%		74.78%
Epoch    47: reducing learning rate of group 0 to 1.0000e-01.
47	0.232		99.00%		75.52%
48	0.219		99.00%		75.97%
*49	0.217		99.00%		76.19%
50	0.218		99.00%		76.04%
*51	0.215		99.00%		76.12%
*52	0.217		99.00%		76.27%
53	0.215		99.00%		75.97%
54	0.218		99.00%		76.19%
55	0.216		99.00%		75.97%
56	0.213		99.00%		76.19%
57	0.212		99.00%		76.12%
58	0.211		99.00%		76.12%
59	0.213		99.00%		76.12%
Epoch    60: reducing learning rate of group 0 to 1.0000e-02.
60	0.212		99.00%		76.19%
61	0.210		99.00%		76.12%
62	0.212		99.00%		76.04%
63	0.211		99.00%		76.12%
64	0.210		99.00%		76.12%
65	0.213		99.00%		75.97%
66	0.207		99.00%		76.12%
67	0.210		99.00%		75.97%
Epoch    68: reducing learning rate of group 0 to 1.0000e-03.
68	0.211		99.00%		76.19%
69	0.213		99.00%		76.12%
70	0.210		99.00%		76.12%
*71	0.213		99.00%		76.19%
Best at epoch 71, test accuaray 76.194030
