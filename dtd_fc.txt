Prepare the network and data.
DataParallel(
  (module): BCNN(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
    )
    (fc): Linear(in_features=9216, out_features=57, bias=True)
    (norm): runLayer()
  )
)
=> loading annotations from: dtd_train.txt ...
=> loading annotations from: dtd_test.txt ...
Training.
Epoch	Train loss	Train acc	Test acc
./src/bilinear_cnn_fc_dtd.py:40: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.fc.weight.data)
./src/bilinear_cnn_fc_dtd.py:41: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w1.data)
./src/bilinear_cnn_fc_dtd.py:42: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w2.data)
./src/bilinear_cnn_fc_dtd.py:43: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w3.data)
./src/bilinear_cnn_fc_dtd.py:44: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w4.data)
./src/bilinear_cnn_fc_dtd.py:47: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  torch.nn.init.constant(self.fc.bias.data, val=0)
1	3.840		8.00%		28.59%
2	3.298		41.00%		47.66%
3	2.808		63.00%		49.52%
4	2.433		65.00%		52.15%
5	2.114		72.00%		57.34%
6	1.871		74.00%		57.31%
7	1.669		78.00%		59.20%
8	1.513		80.00%		60.05%
9	1.390		81.00%		59.87%
10	1.268		84.00%		61.12%
11	1.185		84.00%		61.44%
12	1.094		86.00%		62.10%
13	1.021		88.00%		62.37%
14	0.967		87.00%		62.15%
15	0.899		89.00%		62.37%
16	0.845		90.00%		63.30%
17	0.799		91.00%		63.14%
18	0.754		91.00%		63.40%
19	0.713		92.00%		63.22%
20	0.677		92.00%		63.56%
21	0.641		93.00%		63.70%
22	0.619		94.00%		63.91%
23	0.579		95.00%		64.15%
24	0.566		95.00%		64.34%
25	0.540		95.00%		64.12%
26	0.506		96.00%		64.10%
27	0.484		96.00%		64.52%
28	0.464		97.00%		64.87%
29	0.451		97.00%		64.57%
30	0.432		97.00%		64.92%
*31	0.414		97.00%		64.89%
32	0.397		97.00%		64.71%
33	0.383		98.00%		64.60%
Epoch    34: reducing learning rate of group 0 to 1.0000e-01.
34	0.378		97.00%		64.65%
35	0.353		98.00%		64.73%
36	0.349		98.00%		64.92%
37	0.347		98.00%		64.92%
Epoch    38: reducing learning rate of group 0 to 1.0000e-02.
38	0.342		98.00%		64.92%
39	0.342		98.00%		64.87%
40	0.347		98.00%		64.92%
41	0.345		98.00%		64.87%
Epoch    42: reducing learning rate of group 0 to 1.0000e-03.
42	0.343		98.00%		64.89%
43	0.344		98.00%		64.95%
44	0.347		98.00%		64.92%
45	0.345		98.00%		64.92%
46	0.347		98.00%		64.89%
Epoch    47: reducing learning rate of group 0 to 1.0000e-04.
47	0.344		98.00%		64.92%
48	0.342		98.00%		64.89%
49	0.344		98.00%		64.87%
50	0.340		98.00%		64.92%
Epoch    51: reducing learning rate of group 0 to 1.0000e-05.
*51	0.344		98.00%		64.89%
Best at epoch 51, test accuaray 64.893617
