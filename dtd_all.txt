Prepare the network and data.
DataParallel(
  (module): BCNN(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
    )
    (fc): Linear(in_features=9216, out_features=57, bias=True)
    (norm): runLayer()
  )
)
=> loading annotations from: dtd_train.txt ...
=> loading annotations from: dtd_test.txt ...
Training.
Epoch	Train loss	Train acc	Test acc
./src/bilinear_cnn_all_dtd.py:36: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.fc.weight.data)
./src/bilinear_cnn_all_dtd.py:37: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w1.data)
./src/bilinear_cnn_all_dtd.py:38: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w2.data)
./src/bilinear_cnn_all_dtd.py:39: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w3.data)
./src/bilinear_cnn_all_dtd.py:40: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w4.data)
./src/bilinear_cnn_all_dtd.py:43: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  torch.nn.init.constant(self.fc.bias.data, val=0)
*1	0.505		94.00%		62.23%
2	0.550		91.00%		61.81%
3	0.532		91.00%		61.49%
*4	0.385		95.00%		64.65%
*5	0.299		97.00%		65.74%
*6	0.204		98.00%		66.17%
*7	0.183		99.00%		66.76%
8	0.141		99.00%		66.76%
*9	0.122		99.00%		67.39%
10	0.105		100.00%		67.37%
11	0.091		100.00%		67.29%
*12	0.088		100.00%		67.53%
*13	0.079		100.00%		67.61%
14	0.075		100.00%		67.13%
15	0.070		100.00%		67.58%
*16	0.066		100.00%		67.77%
17	0.062		100.00%		67.55%
18	0.060		100.00%		67.50%
19	0.057		100.00%		67.55%
*20	0.055		100.00%		67.82%
21	0.054		100.00%		67.53%
22	0.052		100.00%		67.77%
*23	0.050		100.00%		67.90%
24	0.048		100.00%		67.87%
*25	0.048		100.00%		68.11%
26	0.047		100.00%		67.87%
27	0.046		100.00%		67.87%
28	0.045		100.00%		67.53%
Epoch    29: reducing learning rate of group 0 to 1.0000e-03.
29	0.043		100.00%		67.71%
30	0.042		100.00%		67.90%
31	0.040		100.00%		67.74%
32	0.040		100.00%		67.71%
Epoch    33: reducing learning rate of group 0 to 1.0000e-04.
33	0.041		100.00%		67.69%
34	0.040		100.00%		67.71%
35	0.040		100.00%		67.74%
36	0.040		100.00%		67.79%
Epoch    37: reducing learning rate of group 0 to 1.0000e-05.
37	0.040		100.00%		67.74%
38	0.040		100.00%		67.66%
39	0.040		100.00%		67.71%
40	0.040		100.00%		67.71%
Epoch    41: reducing learning rate of group 0 to 1.0000e-06.
41	0.040		100.00%		67.71%
42	0.040		100.00%		67.71%
43	0.040		100.00%		67.71%
44	0.040		100.00%		67.71%
Epoch    45: reducing learning rate of group 0 to 1.0000e-07.
45	0.040		100.00%		67.71%
46	0.040		100.00%		67.69%
47	0.040		100.00%		67.71%
48	0.041		100.00%		67.69%
Epoch    49: reducing learning rate of group 0 to 1.0000e-08.
49	0.040		100.00%		67.71%
50	0.040		100.00%		67.74%
Best at epoch 25, test accuaray 68.111702
