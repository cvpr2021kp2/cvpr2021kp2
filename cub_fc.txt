/home/tanyu/CVPR2021_KP2/data/cub200
/home/tanyu/CVPR2021_KP2/model_cub
Prepare the network and data.
DataParallel(
  (module): BCNN(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
    )
    (fc): Linear(in_features=9216, out_features=200, bias=True)
    (norm): runLayer()
  )
)
Files already downloaded and verified.
Files already downloaded and verified.
./src/bilinear_cnn_fc_cub.py:36: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.fc.weight.data)
./src/bilinear_cnn_fc_cub.py:37: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w1.data)
./src/bilinear_cnn_fc_cub.py:38: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w2.data)
./src/bilinear_cnn_fc_cub.py:39: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w3.data)
./src/bilinear_cnn_fc_cub.py:40: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  torch.nn.init.kaiming_normal(self.w4.data)
./src/bilinear_cnn_fc_cub.py:44: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.
  torch.nn.init.constant(self.fc.bias.data, val=0)
*1	5.131		5.00%		21.75%
*2	4.542		31.00%		40.66%
*3	3.979		55.00%		52.57%
*4	3.480		68.00%		58.58%
*5	3.056		73.00%		63.22%
*6	2.678		79.00%		64.96%
*7	2.364		81.00%		67.73%
*8	2.104		85.00%		70.16%
*9	1.883		86.00%		70.94%
*10	1.706		87.00%		71.45%
*11	1.549		89.00%		72.32%
*12	1.428		89.00%		72.56%
*13	1.306		90.00%		73.18%
14	1.213		91.00%		73.14%
*15	1.129		92.00%		73.73%
*16	1.052		92.00%		74.46%
17	0.989		93.00%		74.11%
*18	0.928		93.00%		74.53%
*19	0.882		93.00%		74.82%
20	0.835		94.00%		74.72%
*21	0.787		94.00%		75.25%
*22	0.750		95.00%		75.28%
*23	0.714		95.00%		75.54%
24	0.681		96.00%		75.42%
*25	0.648		95.00%		75.60%
*26	0.622		96.00%		75.84%
*27	0.593		96.00%		75.98%
*28	0.569		96.00%		76.16%
*29	0.550		96.00%		76.18%
*30	0.522		97.00%		76.32%
*31	0.514		97.00%		76.35%
32	0.492		97.00%		76.23%
33	0.477		97.00%		76.27%
*34	0.456		97.00%		76.56%
*35	0.445		98.00%		76.61%
36	0.425		98.00%		76.46%
37	0.413		98.00%		76.61%
38	0.400		98.00%		76.56%
39	0.389		98.00%		76.37%
*40	0.375		98.00%		76.75%
*41	0.366		98.00%		76.84%
42	0.357		98.00%		76.70%
43	0.348		98.00%		76.67%
44	0.333		98.00%		76.82%
45	0.329		98.00%		76.53%
46	0.318		99.00%		76.72%
*47	0.310		99.00%		76.87%
*48	0.303		99.00%		76.94%
49	0.292		99.00%		76.70%
*50	0.284		99.00%		76.98%
*51	0.279		99.00%		77.30%
52	0.276		99.00%		76.94%
53	0.264		99.00%		76.87%
54	0.260		99.00%		77.10%
55	0.254		99.00%		76.89%
56	0.243		99.00%		76.87%
Epoch    57: reducing learning rate of group 0 to 1.0000e-01.
57	0.242		99.00%		77.27%
58	0.235		99.00%		77.20%
59	0.234		99.00%		77.10%
60	0.236		99.00%		77.10%
61	0.235		99.00%		77.06%
62	0.231		99.00%		77.08%
Epoch    63: reducing learning rate of group 0 to 1.0000e-02.
63	0.230		99.00%		76.99%
64	0.231		99.00%		76.99%
65	0.228		99.00%		77.05%
66	0.229		99.00%		77.03%
67	0.231		99.00%		76.98%
68	0.233		99.00%		77.13%
Epoch    69: reducing learning rate of group 0 to 1.0000e-03.
69	0.233		99.00%		77.01%
70	0.234		99.00%		76.96%
71	0.230		99.00%		77.06%
72	0.234		99.00%		77.01%
73	0.229		99.00%		77.03%
74	0.228		99.00%		76.99%
Epoch    75: reducing learning rate of group 0 to 1.0000e-04.
75	0.233		99.00%		77.03%
76	0.231		99.00%		76.91%
77	0.233		99.00%		76.98%
78	0.229		99.00%		76.99%
79	0.233		99.00%		77.03%
80	0.232		99.00%		77.01%
Epoch    81: reducing learning rate of group 0 to 1.0000e-05.
*81	0.227		99.00%		76.99%
Best at epoch 81, test accuaray 76.993441
