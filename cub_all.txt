nohup: ignoring input
Prepare the network and data.
DataParallel(
  (module): BCNN(
    (features): Sequential(
      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU(inplace=True)
      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU(inplace=True)
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (6): ReLU(inplace=True)
      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (8): ReLU(inplace=True)
      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (11): ReLU(inplace=True)
      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (13): ReLU(inplace=True)
      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (15): ReLU(inplace=True)
      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (18): ReLU(inplace=True)
      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (20): ReLU(inplace=True)
      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (22): ReLU(inplace=True)
      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (25): ReLU(inplace=True)
      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (27): ReLU(inplace=True)
      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (29): ReLU(inplace=True)
    )
    (fc): Linear(in_features=9216, out_features=200, bias=True)
    (norm): runLayer()
  )
)
Files already downloaded and verified.
Files already downloaded and verified.
Training.
Epoch	Train loss	Train acc	Test acc
*1	0.650		89.00%		74.08%
*2	0.441		93.00%		79.55%
*3	0.266		97.00%		81.72%
*4	0.208		98.00%		81.79%
*5	0.144		99.00%		82.83%
6	0.121		99.00%		82.69%
*7	0.118		99.00%		83.28%
*8	0.102		99.00%		83.48%
*9	0.092		99.00%		84.02%
*10	0.086		99.00%		84.04%
11	0.085		99.00%		83.90%
*12	0.087		99.00%		84.19%
*13	0.085		99.00%		84.93%
14	0.088		99.00%		84.31%
*15	0.088		99.00%		84.97%
16	0.086		99.00%		84.24%
17	0.089		99.00%		84.73%
18	0.090		99.00%		84.09%
19	0.096		99.00%		84.67%
20	0.101		99.00%		84.55%
Epoch    21: reducing learning rate of group 0 to 1.0000e-03.
21	0.103		99.00%		84.19%
*22	0.083		99.00%		85.23%
*23	0.078		100.00%		85.33%
24	0.079		99.00%		85.33%
*25	0.078		99.00%		85.52%
26	0.078		99.00%		85.28%
27	0.078		99.00%		85.43%
28	0.075		100.00%		85.36%
29	0.078		100.00%		85.31%
30	0.074		99.00%		85.43%
Epoch    31: reducing learning rate of group 0 to 1.0000e-04.
31	0.076		100.00%		85.40%
32	0.076		99.00%		85.30%
33	0.075		99.00%		85.33%
34	0.075		99.00%		85.28%
35	0.076		99.00%		85.23%
36	0.077		99.00%		85.31%
Epoch    37: reducing learning rate of group 0 to 1.0000e-05.
37	0.076		99.00%		85.33%
38	0.076		99.00%		85.35%
39	0.076		100.00%		85.31%
40	0.076		99.00%		85.35%
41	0.075		99.00%		85.36%
42	0.076		99.00%		85.36%
Epoch    43: reducing learning rate of group 0 to 1.0000e-06.
43	0.075		99.00%		85.33%
44	0.075		99.00%		85.35%
45	0.077		99.00%		85.35%
46	0.072		99.00%		85.35%
47	0.074		99.00%		85.35%
48	0.074		99.00%		85.36%
Epoch    49: reducing learning rate of group 0 to 1.0000e-07.
49	0.075		99.00%		85.36%
50	0.076		99.00%		85.36%
Best at epoch 25, test accuaray 85.519503
